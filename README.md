# ğŸ¥ Frontend - SignalForHelp  

Este proyecto implementa una **aplicaciÃ³n en React** que captura video desde la cÃ¡mara del usuario, envÃ­a frames al **backend vÃ­a WebSocket**, y muestra en tiempo real las predicciones de un modelo **YOLOv8** (palma / puÃ±o) junto con la imagen procesada.  

---

## âš™ï¸ Requisitos

- Node.js **16+**
- npm o yarn

---

## ğŸ“¥ InstalaciÃ³n

1. Clona el repositorio:

```bash
git clone https://github.com/tuusuario/desarrollo-proyecto-signalForHelp-frontend.git
cd desarrollo-proyecto-signalForHelp-frontend
```

2. Instala dependencias:

```bash
npm install
```

o con yarn:

```bash
yarn install
```

---

## ğŸš€ EjecuciÃ³n

Ejecuta el proyecto en modo desarrollo:

```bash
npm start
```

La aplicaciÃ³n estarÃ¡ disponible en:

```
http://localhost:3000
```

---

## ğŸ“‚ Estructura principal

```
desarrollo-proyecto-signalForHelp-frontend/
â”œâ”€â”€ public/ # Archivos estÃ¡ticos (ej: Ã­conos, imÃ¡genes pÃºblicas)
â”œâ”€â”€ src/ # CÃ³digo fuente principal
â”‚ â”œâ”€â”€ assets/ # Recursos estÃ¡ticos (ej: imÃ¡genes internas como react.svg)
â”‚ â”œâ”€â”€ components/ # Componentes principales de la UI
â”‚ â”‚ â”œâ”€â”€ Navbar.jsx
â”‚ â”‚ â”œâ”€â”€ UploadImage.jsx
â”‚ â”‚ â””â”€â”€ VideoStream.jsx
â”‚ â”œâ”€â”€ services/ # Servicios y utilidades (ej: api.js para peticiones)
â”‚ â”œâ”€â”€ App.jsx # Componente raÃ­z
â”‚ â”œâ”€â”€ App.css # Estilos globales de la app
â”‚ â”œâ”€â”€ index.css # Estilos base
â”‚ â””â”€â”€ main.jsx # Punto de entrada de la aplicaciÃ³n
â”œâ”€â”€ tests/ # Pruebas unitarias y de integraciÃ³n
â”‚ â””â”€â”€ App.test.jsx
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile # ConfiguraciÃ³n para ejecuciÃ³n en contenedor
â”œâ”€â”€ eslint.config.js # ConfiguraciÃ³n de ESLint
â”œâ”€â”€ index.html # HTML base
â”œâ”€â”€ package.json # Dependencias y scripts
â”œâ”€â”€ package-lock.json
â”œâ”€â”€ vite.config.js # ConfiguraciÃ³n de Vite
â””â”€â”€ README.md # Este archivo
```

---

## ğŸ“¡ Flujo de trabajo

1. El componente **`VideoStream.jsx`**:
   - Captura video con `navigator.mediaDevices.getUserMedia`.
   - Escala los frames (ej: 320x240).
   - Comprime a JPEG.
   - EnvÃ­a los bytes al **backend** vÃ­a **WebSocket** en:  
     ```
     ws://127.0.0.1:8000/api/segment-video
     ```
2. El backend procesa cada frame con YOLO y responde un JSON:

```json
{
  "image_base64": ".....",
  "detections": [
    {
      "class_id": 0,
      "class_name": "palma",
      "confidence": 0.92,
      "bbox": [34.1, 50.3, 200.4, 300.7]
    }
  ]
}
```

3. El frontend:
   - Muestra la imagen procesada (`image_base64`).
   - Lista las detecciones (`detections`).

---

## ğŸ› ï¸ Notas

- Puedes ajustar **frecuencia de envÃ­o (`setInterval`)** y **resoluciÃ³n (`canvas.width/height`)** para optimizar rendimiento.
- Si usas **backend en otra mÃ¡quina/puerto**, cambia la URL del WebSocket:
  ```js
  const socket = new WebSocket("ws://TU_IP:8000/api/segment-video");
  ```
- Para producciÃ³n: construir el proyecto con
  ```bash
  npm run build
  ```